{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "a='This is good movies. This is bad as that movie'\n",
    "d={}\n",
    "\n",
    "for token  in nltk.word_tokenize(a):\n",
    "    if token not in d:\n",
    "        d[token] = 1\n",
    "    else :\n",
    "        d[token]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['this is it it came all way',\n",
    "            'he should have gone by this time',\n",
    "            'is this really him talking',\n",
    "             'i mad sure he tell lies',\n",
    "             'something is strange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is it it came all way'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need unique words to define the length of vocabulary\n",
    "\n",
    "unique_words = set()\n",
    "for sent in sentences:\n",
    "    for word in nltk.word_tokenize(sent):\n",
    "        unique_words.add(word)\n",
    "\n",
    "vocab={}\n",
    "\n",
    "for i , word in enumerate(sorted(list(unique_words))):    \n",
    "    vocab[word]=i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': 0,\n",
       " 'by': 1,\n",
       " 'came': 2,\n",
       " 'gone': 3,\n",
       " 'have': 4,\n",
       " 'he': 5,\n",
       " 'him': 6,\n",
       " 'i': 7,\n",
       " 'is': 8,\n",
       " 'it': 9,\n",
       " 'lies': 10,\n",
       " 'mad': 11,\n",
       " 'really': 12,\n",
       " 'should': 13,\n",
       " 'something': 14,\n",
       " 'strange': 15,\n",
       " 'sure': 16,\n",
       " 'talking': 17,\n",
       " 'tell': 18,\n",
       " 'this': 19,\n",
       " 'time': 20,\n",
       " 'way': 21}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "row=[]\n",
    "col=[]\n",
    "val=[]\n",
    "for idx,sent in enumerate(sentences):\n",
    "    count_vector=dict(Counter(sent.split(' ')))\n",
    "    \n",
    "    for word,count in count_vector.items():\n",
    "    \n",
    "        row.append(idx)\n",
    "        col.append(vocab.get(word)) \n",
    "        val.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4]\n",
      "[19, 8, 9, 2, 0, 21, 5, 13, 4, 3, 1, 19, 20, 8, 19, 12, 6, 17, 7, 11, 16, 5, 18, 10, 14, 8, 15]\n",
      "[1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(row)\n",
    "print(col)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      " [0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(csr_matrix((val,(row,col))).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diadvantage of COuntVetorizer\n",
    "'''\n",
    "*Its inability in identifying more important and less important words for analysis.\n",
    "*It also doesn't identify the relationships between words such as linguistic similarity between words.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SKlearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are', 'doing', 'good', 'hi', 'hope', 'how', 'you']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=['Hi How are you','Hope you are doing good']\n",
    "\n",
    "cv=CountVectorizer()\n",
    "cv.fit(documents)\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector=cv.fit_transform(documents).toarray()\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>doing</th>\n",
       "      <th>good</th>\n",
       "      <th>hi</th>\n",
       "      <th>hope</th>\n",
       "      <th>how</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  doing  good  hi  hope  how  you\n",
       "0    1      0     0   1     0    1    1\n",
       "1    1      1     1   0     1    0    1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data=vector,columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF\n",
    "\n",
    "#What if we want words that are common but are not in abundant and not too rare,which shows the imporatnce of words\n",
    "termfreq= frequency frqwuncy is the number of times the word i has occured in the document j to the total number of words in documents\n",
    "\n",
    "log inverse= total number of documents to the documents that contain that word\n",
    "\n",
    "TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present\n",
    "in the corpus but also provides the importance of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[\"The car is driven on the road\",\"The truck is driven on the highway\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'driven', 'highway', 'is', 'on', 'road', 'the', 'truck']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf=TfidfVectorizer()\n",
    "tf.fit(documents)\n",
    "tf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.42471719, 0.30218978, 0.        , 0.30218978, 0.30218978,\n",
       "        0.42471719, 0.60437955, 0.        ],\n",
       "       [0.        , 0.30218978, 0.42471719, 0.30218978, 0.30218978,\n",
       "        0.        , 0.60437955, 0.42471719]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the', 'The', 'driven', 'car', 'is', 'road', 'on', 'highway', 'truck'}\n"
     ]
    }
   ],
   "source": [
    "docA = \"The car is driven on the road\"\n",
    "\n",
    "docB = \"The truck is driven on the highway\"\n",
    "\n",
    "token_A = nltk.word_tokenize(docA)\n",
    "token_B = nltk.word_tokenize(docB)\n",
    "\n",
    "unique_words = set(token_A).union(set(token_B))\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TF(token_doc):\n",
    "    \n",
    "    bow_doc =dict(Counter(token_doc))\n",
    "    print('bagOfWords--->', bow_doc)\n",
    "    \n",
    "    len_doc = len(token_doc)\n",
    "    print('length of document-->',len_doc)\n",
    "    \n",
    "    tf_doc = { key : bow_doc[key]/len_doc for key in bow_doc.keys()}\n",
    "    \n",
    "    return tf_doc,bow_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagOfWords---> {'The': 1, 'car': 1, 'is': 1, 'driven': 1, 'on': 1, 'the': 1, 'road': 1}\n",
      "length of document--> 7\n",
      "bagOfWords---> {'The': 1, 'truck': 1, 'is': 1, 'driven': 1, 'on': 1, 'the': 1, 'highway': 1}\n",
      "length of document--> 7\n",
      "TF_A----> {'The': 0.14285714285714285, 'car': 0.14285714285714285, 'is': 0.14285714285714285, 'driven': 0.14285714285714285, 'on': 0.14285714285714285, 'the': 0.14285714285714285, 'road': 0.14285714285714285}\n",
      "TF_B----> {'The': 0.14285714285714285, 'truck': 0.14285714285714285, 'is': 0.14285714285714285, 'driven': 0.14285714285714285, 'on': 0.14285714285714285, 'the': 0.14285714285714285, 'highway': 0.14285714285714285}\n"
     ]
    }
   ],
   "source": [
    "TF_A , BOW_A = compute_TF(token_A)\n",
    "TF_B , BOW_B= compute_TF(token_B)\n",
    "\n",
    "\n",
    "print('TF_A---->',TF_A)\n",
    "print('TF_B---->',TF_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compuet_IDF(no_of_doc):\n",
    "    import math\n",
    "    total={}\n",
    "    N=len(no_of_doc)\n",
    "    for doc in no_of_doc:\n",
    "        for key in doc.keys():\n",
    "            if key in total:\n",
    "                total[key]+=1\n",
    "            else:\n",
    "                total[key]=1\n",
    "    print(total)\n",
    "    for key, val in total.items():\n",
    "        total[key] = math.log(N / float(val))\n",
    "    \n",
    "    \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 2, 'car': 1, 'is': 2, 'driven': 2, 'on': 2, 'the': 2, 'road': 1, 'truck': 1, 'highway': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'The': 0.0,\n",
       " 'car': 0.6931471805599453,\n",
       " 'is': 0.0,\n",
       " 'driven': 0.0,\n",
       " 'on': 0.0,\n",
       " 'the': 0.0,\n",
       " 'road': 0.6931471805599453,\n",
       " 'truck': 0.6931471805599453,\n",
       " 'highway': 0.6931471805599453}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf=compuet_IDF([BOW_A,BOW_B])\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TFIDF(TF_doc,idf):\n",
    "    tfidf = {}\n",
    "    for key, val in TF_doc.items():\n",
    "        tfidf[key] = val * idf[key]\n",
    "    return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0.0, 'car': 0.09902102579427789, 'is': 0.0, 'driven': 0.0, 'on': 0.0, 'the': 0.0, 'road': 0.09902102579427789}\n",
      "{'The': 0.0, 'truck': 0.09902102579427789, 'is': 0.0, 'driven': 0.0, 'on': 0.0, 'the': 0.0, 'highway': 0.09902102579427789}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The</th>\n",
       "      <th>car</th>\n",
       "      <th>is</th>\n",
       "      <th>driven</th>\n",
       "      <th>on</th>\n",
       "      <th>the</th>\n",
       "      <th>road</th>\n",
       "      <th>truck</th>\n",
       "      <th>highway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.099021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   The       car   is  driven   on  the      road     truck   highway\n",
       "0  0.0  0.099021  0.0     0.0  0.0  0.0  0.099021       NaN       NaN\n",
       "1  0.0       NaN  0.0     0.0  0.0  0.0       NaN  0.099021  0.099021"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_A=compute_TFIDF(TF_A,idf)\n",
    "print(tfidf_A)\n",
    "tfidf_B=compute_TFIDF(TF_B,idf)\n",
    "print(tfidf_B)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([tfidf_A, tfidf_B])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42471719 0.30218978 0.         0.30218978 0.30218978 0.42471719\n",
      "  0.60437955 0.        ]\n",
      " [0.         0.30218978 0.42471719 0.30218978 0.30218978 0.\n",
      "  0.60437955 0.42471719]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([docA,docB])\n",
    "print(X.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
